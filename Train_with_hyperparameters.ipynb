{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYgMfT_1btnP",
        "outputId": "87c08eab-e7e3-4635-d4b8-09466e764b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('/content/drive/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF2E9t45fWFo",
        "outputId": "6e43c671-3d77-4501-ef99-bb0a25f79290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=3.08s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Epoch [1/5] - Train Loss: 0.6861 - Val Loss: 0.6808\n",
            "Epoch [2/5] - Train Loss: 0.5108 - Val Loss: 0.6800\n",
            "Epoch [3/5] - Train Loss: 0.4174 - Val Loss: 0.6789\n",
            "Epoch [4/5] - Train Loss: 0.3625 - Val Loss: 0.6772\n",
            "Epoch [5/5] - Train Loss: 0.3143 - Val Loss: 0.6748\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "images_dir = \"/content/drive/MyDrive/val2017\"\n",
        "masks_dir = \"/content/drive/MyDrive/Masks\"\n",
        "out_mask_dir = \"/content/drive/MyDrive/val2017/masks_png\"\n",
        "os.makedirs(out_mask_dir, exist_ok=True)\n",
        "\n",
        "# --- Generate masks from COCO JSON ---\n",
        "json_files = [f for f in os.listdir(masks_dir) if f.endswith(\".json\")]\n",
        "for json_file in json_files:\n",
        "    ann_path = os.path.join(masks_dir, json_file)\n",
        "    coco = COCO(ann_path)\n",
        "    img_ids = coco.getImgIds()\n",
        "    for img_id in img_ids:\n",
        "        img_info = coco.loadImgs(img_id)[0]\n",
        "        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n",
        "        mask = np.zeros((img_info[\"height\"], img_info[\"width\"]), dtype=np.uint8)\n",
        "        for ann in anns:\n",
        "            mask = np.maximum(mask, coco.annToMask(ann) * 255)\n",
        "        mask_img = Image.fromarray(mask)\n",
        "        mask_name = img_info['file_name'].replace(\".jpg\", \".png\")\n",
        "        mask_img.save(os.path.join(out_mask_dir, mask_name))\n",
        "\n",
        "\n",
        "# --- Dataset ---\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform_img=None, transform_mask=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_files = sorted(os.listdir(images_dir))\n",
        "        self.mask_files = sorted(os.listdir(masks_dir))\n",
        "        self.valid_pairs = []\n",
        "        for img_name in self.image_files:\n",
        "            mask_name = img_name.replace(\".jpg\", \".png\")\n",
        "            if mask_name in self.mask_files:\n",
        "                self.valid_pairs.append((img_name, mask_name))\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_mask = transform_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, mask_name = self.valid_pairs[idx]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        if self.transform_img:\n",
        "            image = self.transform_img(image)\n",
        "        if self.transform_mask:\n",
        "            mask = self.transform_mask(mask)\n",
        "            mask = (mask > 0).float()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "# --- Transforms ---\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "transform_mask = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = SegmentationDataset(images_dir=images_dir,\n",
        "                              masks_dir=out_mask_dir,\n",
        "                              transform_img=transform_img,\n",
        "                              transform_mask=transform_mask)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "\n",
        "# --- UNet blocks ---\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_channels, out_channels))\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX//2, diffX-diffX//2,\n",
        "                        diffY//2, diffY-diffY//2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "# --- UNet model ---\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024 + 512, 512)\n",
        "        self.up2 = Up(512 + 256, 256)\n",
        "        self.up3 = Up(256 + 128, 128)\n",
        "        self.up4 = Up(128 + 64, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        return self.outc(x)\n",
        "\n",
        "model = UNet(n_channels=3, n_classes=1).to(device)\n",
        "\n",
        "\n",
        "# --- Training loop ---\n",
        "def train_model(model, train_loader, val_loader, num_epochs=5):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for images, masks in train_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)\n",
        "                val_loss += criterion(outputs, masks).item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "              f\"Train Loss: {avg_loss:.4f} - \"\n",
        "              f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- Train for 5 epochs ---\n",
        "trained_model = train_model(model, train_loader, val_loader, num_epochs=5)\n",
        "torch.save(trained_model.state_dict(), \"unet_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pWs1evFgEWe",
        "outputId": "dcaee027-7ea7-4c83-d940-43aff74badb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepLabV3 model saved.\n"
          ]
        }
      ],
      "source": [
        "torch.save(trained_model.state_dict(), \"deeplabv3_model.pth\")\n",
        "print(\"DeepLabV3 model saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hyperparameters"
      ],
      "metadata": {
        "id": "1hDZTICypKIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "\n",
        "dropout_rate = 0.3\n",
        "image_size = (128, 128)\n",
        "num_epochs = 5\n",
        "batch_size = 8\n",
        "learning_rate = 1e-4\n",
        "\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "transform_mask = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform_img=None, transform_mask=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(\".jpg\") or f.endswith(\".png\")])\n",
        "        self.mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith(\".png\")])\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_mask = transform_mask\n",
        "        self.valid_pairs = []\n",
        "        for img_file in self.image_files:\n",
        "            mask_file = img_file.replace(\".jpg\", \".png\")\n",
        "            if mask_file in self.mask_files:\n",
        "                self.valid_pairs.append((img_file, mask_file))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file, mask_file = self.valid_pairs[idx]\n",
        "        img = Image.open(os.path.join(self.images_dir, img_file)).convert(\"RGB\")\n",
        "        mask = Image.open(os.path.join(self.masks_dir, mask_file)).convert(\"L\")\n",
        "        if self.transform_img:\n",
        "            img = self.transform_img(img)\n",
        "        if self.transform_mask:\n",
        "            mask = self.transform_mask(mask)\n",
        "        return img, mask\n",
        "\n",
        "train_dataset = SegmentationDataset(\n",
        "    images_dir=\"/content/drive/MyDrive/val2017\",\n",
        "    masks_dir=\"/content/drive/MyDrive/val2017/masks_png\",\n",
        "    transform_img=transform_img,\n",
        "    transform_mask=transform_mask\n",
        ")\n",
        "val_dataset = SegmentationDataset(\n",
        "    images_dir=\"/content/drive/MyDrive/val2017\",\n",
        "    masks_dir=\"/content/drive/MyDrive/val2017/masks_png\",\n",
        "    transform_img=transform_img,\n",
        "    transform_mask=transform_mask\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class UNetDropout(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.dropout = nn.Dropout2d(p=dropout)\n",
        "        self.up1 = Up(1024 + 512, 512)\n",
        "        self.up2 = Up(512 + 256, 256)\n",
        "        self.up3 = Up(256 + 128, 128)\n",
        "        self.up4 = Up(128 + 64, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x5 = self.dropout(x5)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        return self.outc(x)\n",
        "\n",
        "def train_val(model, train_loader, val_loader, num_epochs, optimizer, criterion, device):\n",
        "    history = {\"train_loss\": [], \"val_loss\": []}\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, masks in train_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        history[\"val_loss\"].append(avg_val_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "    return history\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "results = []\n",
        "\n",
        "optimizers = {\n",
        "    \"Adam\": lambda params: optim.Adam(params, lr=learning_rate),\n",
        "    \"SGD\": lambda params: optim.SGD(params, lr=learning_rate, momentum=0.9),\n",
        "    \"RMSprop\": lambda params: optim.RMSprop(params, lr=learning_rate)\n",
        "}\n",
        "\n",
        "for opt_name, opt_fn in optimizers.items():\n",
        "    print(f\"\\nRunning {opt_name}\")\n",
        "    model = UNetDropout(n_channels=3, n_classes=1, dropout=dropout_rate).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = opt_fn(model.parameters())\n",
        "    start = time.time()\n",
        "    history = train_val(model, train_loader, val_loader, num_epochs, optimizer, criterion, device)\n",
        "    end = time.time()\n",
        "    results.append({\n",
        "        \"Optimizer\": opt_name,\n",
        "        \"LR\": learning_rate,\n",
        "        \"Batch Size\": batch_size,\n",
        "        \"Epochs\": num_epochs,\n",
        "        \"Dropout\": dropout_rate,\n",
        "        \"Final Val Loss\": history[\"val_loss\"][-1],\n",
        "        \"Training Time (s)\": round(end - start, 2)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(\"\\nResults Summary\")\n",
        "print(df)\n",
        "df.to_csv(\"hyperparam_results.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbQO2UqYdqZn",
        "outputId": "9647e129-ee01-45af-dc0f-9e7fe756a362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Adam\n",
            "Epoch [1/5] - Train Loss: 0.8232, Val Loss: 0.7251\n",
            "Epoch [2/5] - Train Loss: 0.8092, Val Loss: 0.7306\n",
            "Epoch [3/5] - Train Loss: 0.7773, Val Loss: 0.7355\n",
            "Epoch [4/5] - Train Loss: 0.7389, Val Loss: 0.7396\n",
            "Epoch [5/5] - Train Loss: 0.7154, Val Loss: 0.7434\n",
            "\n",
            "Running SGD\n",
            "Epoch [1/5] - Train Loss: 0.7027, Val Loss: 0.6600\n",
            "Epoch [2/5] - Train Loss: 0.6996, Val Loss: 0.6617\n",
            "Epoch [3/5] - Train Loss: 0.7008, Val Loss: 0.6618\n",
            "Epoch [4/5] - Train Loss: 0.6998, Val Loss: 0.6607\n",
            "Epoch [5/5] - Train Loss: 0.6983, Val Loss: 0.6595\n",
            "\n",
            "Running RMSprop\n",
            "Epoch [1/5] - Train Loss: 1.0305, Val Loss: 0.7367\n",
            "Epoch [2/5] - Train Loss: 0.9327, Val Loss: 0.7406\n",
            "Epoch [3/5] - Train Loss: 0.7797, Val Loss: 1.2551\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}