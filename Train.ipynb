{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "16_7GhT9bd8M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('/content/drive/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYgMfT_1btnP",
        "outputId": "a630dc8b-f0ea-47e2-bbe0-afa85ec1ddda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024+512, 512)\n",
        "        self.up2 = Up(512+256, 256)\n",
        "        self.up3 = Up(256+128, 128)\n",
        "        self.up4 = Up(128+64, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model\n",
        "model = UNet(n_channels=3, n_classes=1)\n",
        "\n",
        "print(\"Corrected UNet model architecture defined and instantiated.\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPcpvza2b0tt",
        "outputId": "33affd70-7a1e-46d6-8d74-873b6e99fe86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected UNet model architecture defined and instantiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024+512, 512)\n",
        "        self.up2 = Up(512+256, 256)\n",
        "        self.up3 = Up(256+128, 128)\n",
        "        self.up4 = Up(128+64, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "model = UNet(n_channels=3, n_classes=1)"
      ],
      "metadata": {
        "id": "96blyNwgcCKm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "ann_dir = \"/content/drive/MyDrive/Masks\"\n",
        "img_dir = \"/content/drive/MyDrive/val2017\"\n",
        "out_mask_dir = \"/content/drive/MyDrive/val2017/masks_png\"\n",
        "os.makedirs(out_mask_dir, exist_ok=True)\n",
        "\n",
        "json_files = [f for f in os.listdir(ann_dir) if f.endswith(\".json\")]\n",
        "\n",
        "for json_file in json_files:\n",
        "    ann_path = os.path.join(ann_dir, json_file)\n",
        "    coco = COCO(ann_path)\n",
        "\n",
        "    img_ids = coco.getImgIds()\n",
        "    for img_id in img_ids:\n",
        "        img_info = coco.loadImgs(img_id)[0]\n",
        "        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n",
        "\n",
        "        mask = np.zeros((img_info[\"height\"], img_info[\"width\"]), dtype=np.uint8)\n",
        "\n",
        "        for ann in anns:\n",
        "            mask = np.maximum(mask, coco.annToMask(ann) * 255)\n",
        "        mask_img = Image.fromarray(mask)\n",
        "        mask_name = img_info['file_name'].replace(\".jpg\", \".png\")\n",
        "        mask_img.save(os.path.join(out_mask_dir, mask_name))\n",
        "\n",
        "print(\"Mask generation completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP-K-7UMcwpG",
        "outputId": "806cb737-7245-4863-c827-e5c5d9352514"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.55s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.31s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.35s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.44s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.82s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.29s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.52s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.18s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.19s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.49s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.54s)\n",
            "creating index...\n",
            "index created!\n",
            "Mask generation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "images_dir = \"/content/drive/MyDrive/val2017\"\n",
        "masks_dir = \"/content/drive/MyDrive/Masks\"\n",
        "out_mask_dir = \"/content/drive/MyDrive/val2017/masks_png\"\n",
        "os.makedirs(out_mask_dir, exist_ok=True)\n",
        "\n",
        "json_files = [f for f in os.listdir(masks_dir) if f.endswith(\".json\")]\n",
        "for json_file in json_files:\n",
        "    ann_path = os.path.join(masks_dir, json_file)\n",
        "    coco = COCO(ann_path)\n",
        "    img_ids = coco.getImgIds()\n",
        "    for img_id in img_ids:\n",
        "        img_info = coco.loadImgs(img_id)[0]\n",
        "        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n",
        "        mask = np.zeros((img_info[\"height\"], img_info[\"width\"]), dtype=np.uint8)\n",
        "        for ann in anns:\n",
        "            mask = np.maximum(mask, coco.annToMask(ann) * 255)\n",
        "        mask_img = Image.fromarray(mask)\n",
        "        mask_name = img_info['file_name'].replace(\".jpg\", \".png\")\n",
        "        mask_img.save(os.path.join(out_mask_dir, mask_name))\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_files = sorted(os.listdir(images_dir))\n",
        "        self.mask_files = sorted(os.listdir(masks_dir))\n",
        "        self.valid_pairs = []\n",
        "        for img_name in self.image_files:\n",
        "            mask_name = img_name.replace(\".jpg\", \".png\")\n",
        "            if mask_name in self.mask_files:\n",
        "                self.valid_pairs.append((img_name, mask_name))\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.valid_pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, mask_name = self.valid_pairs[idx]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "        return image, mask\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()])\n",
        "dataset = SegmentationDataset(images_dir=images_dir, masks_dir=out_mask_dir, transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_channels, out_channels))\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX//2, diffX-diffX//2, diffY//2, diffY-diffY//2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64,128)\n",
        "        self.down2 = Down(128,256)\n",
        "        self.down3 = Down(256,512)\n",
        "        self.down4 = Down(512,1024)\n",
        "        self.up1 = Up(1024+512,512)\n",
        "        self.up2 = Up(512+256,256)\n",
        "        self.up3 = Up(256+128,128)\n",
        "        self.up4 = Up(128+64,64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5,x4)\n",
        "        x = self.up2(x,x3)\n",
        "        x = self.up3(x,x2)\n",
        "        x = self.up4(x,x1)\n",
        "        return self.outc(x)\n",
        "\n",
        "model = UNet(n_channels=3,n_classes=1).to(device)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=5):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for images, masks in train_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss/len(train_loader)\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)\n",
        "                val_loss += criterion(outputs, masks).item()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_loss:.4f} - Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, num_epochs=5)\n",
        "torch.save(trained_model.state_dict(), \"unet_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZxIueQDcWbn",
        "outputId": "b34e8fc7-f099-4fc2-8024-ae3ffb00aa92"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Epoch [1/5] - Train Loss: 0.7409 - Val Loss: 0.7479\n",
            "Epoch [2/5] - Train Loss: 0.7120 - Val Loss: 0.7432\n",
            "Epoch [3/5] - Train Loss: 0.6780 - Val Loss: 0.7394\n",
            "Epoch [4/5] - Train Loss: 0.6420 - Val Loss: 0.7368\n",
            "Epoch [5/5] - Train Loss: 0.6107 - Val Loss: 0.7343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.save(trained_model.state_dict(), \"my_unet_model.pth\")\n",
        "print(\"Model saved as my_unet_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjEtzdW7fINR",
        "outputId": "0f2cd43f-9c8e-4f12-8867-72e1aafd6955"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as my_unet_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "model = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
        "model.classifier[4] = nn.Conv2d(256, 1, kernel_size=1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Pre-trained model loaded and final layer adjusted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJW9FbIBfO7Y",
        "outputId": "d220b401-480b-4c2e-f939-488679ca0513"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161M/161M [00:02<00:00, 65.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained model loaded and final layer adjusted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 5\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for images, masks in train_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)[\"out\"]\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)[\"out\"]\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "trained_model = train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF2E9t45fWFo",
        "outputId": "b73e2976-6642-4d39-f081-d97071f20a2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] - Train Loss: 0.9929, Val Loss: 1.0079\n",
            "Epoch [2/5] - Train Loss: 0.9753, Val Loss: 1.0224\n",
            "Epoch [3/5] - Train Loss: 0.9494, Val Loss: 1.0294\n",
            "Epoch [4/5] - Train Loss: 0.9242, Val Loss: 1.0364\n",
            "Epoch [5/5] - Train Loss: 0.9031, Val Loss: 1.0392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(trained_model.state_dict(), \"deeplabv3_model.pth\")\n",
        "print(\"DeepLabV3 model saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pWs1evFgEWe",
        "outputId": "56e5f677-8eb9-4bdc-9f32-b6eb9f2423fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepLabV3 model saved.\n"
          ]
        }
      ]
    }
  ]
}