{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b961e28-af5e-4ee8-80f2-82306bdf80fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4288478900.py, line 56)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimages_path = C:\\Users\\KIIT0001\\Desktop\\infosys aiml\\val2017\\val2017\u001b[39m\n                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "class CocoMaskedDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_path, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.coco = COCO(annotations_path)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.images_path, img_info['file_name'])\n",
    "\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "        for ann in anns:\n",
    "            m = self.coco.annToMask(ann)\n",
    "            mask = np.maximum(mask, m)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "\n",
    "        return img, mask.long()\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(p=0.3),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images_path = C:\\Users\\KIIT0001\\Desktop\\infosys aiml\\val2017\\val2017\n",
    "annotations_path =\"C:\\Users\\KIIT0001\\Desktop\\infosys aiml\\annotations_trainval2017\\annotations\\instances_val2017.json\"\n",
    "\n",
    "dataset = CocoMaskedDataset(images_path, annotations_path, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)[\"out\"]\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)[\"out\"]\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "trained_model = train_model(model, train_loader, val_loader, num_epochs=5)\n",
    "torch.save(trained_model.state_dict(), \"deeplabv3_hand_segmentation.pth\")\n",
    "print(\"âœ… Model saved as deeplabv3_hand_segmentation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee303bc3-9a2d-48c6-9183-d733e0b720a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce167c-7929-4c32-bec6-7aa0a7068993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
